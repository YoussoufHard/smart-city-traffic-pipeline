# Smart City Traffic Pipeline
*Projet de Synth√®se - Big Data Processing*

Ce projet impl√©mente un pipeline Big Data complet pour la collecte, le traitement et l'analyse de donn√©es de trafic urbain en temps r√©el. Il r√©pond √† la probl√©matique de la gestion intelligente de la mobilit√© dans une Smart City.

---

## üìö Table des Mati√®res
1. [Introduction & Contexte](#1-introduction--contexte)
2. [Architecture Technique](#2-architecture-technique)
3. [Installation & D√©marrage (Quick Start)](#3-installation--d√©marrage-quick-start)
4. [Rapport de Projet D√©taill√©](#4-rapport-de-projet-d√©taill√©)
    - [√âtape 1 : Collecte des Donn√©es](#√©tape-1--collecte-des-donn√©es-data-collection)
    - [√âtape 2 : Ingestion (Kafka)](#√©tape-2--ingestion-des-donn√©es-data-ingestion)
    - [√âtape 3 : Data Lake (HDFS)](#√©tape-3--stockage-des-donn√©es-brutes-data-lake--raw-zone)
    - [√âtape 4 : Traitement (Spark)](#√©tape-4--traitement-des-donn√©es-data-processing)
    - [√âtape 5 : Zone Analytique](#√©tape-5--structuration-analytique-analytics-zone)
    - [√âtape 6 : Visualisation (Grafana)](#√©tape-6--exploitation-et-visualisation)
    - [√âtape 7 : Orchestration (Airflow)](#√©tape-7--orchestration-du-pipeline)

---

## 1. Introduction & Contexte
Dans le cadre des **Smart Cities**, la surveillance du trafic est cruciale. Ce projet vise √† :
- Surveiller l'√©tat du trafic en temps r√©el (Vitesse, Densit√©).
- D√©tecter les congestions automatiquement.
- Stocker et historiser les donn√©es pour analyse future.

**Probl√©matique :** *Comment concevoir et impl√©menter un pipeline Big Data capable de collecter des donn√©es de trafic urbain en temps r√©el, de les stocker, de les traiter et de produire des indicateurs exploitables ?*

---

## 2. Architecture Technique
Le projet repose sur une stack moderne et conteneuris√©e :

```mermaid
graph LR
    subgraph "Source (IoT)"
        A[G√©n√©rateur Python] -->|JSON| B(Kafka)
    end
    
    subgraph "Ingestion & Stockage"
        B -->|Consumer| C[HDFS Raw Zone]
    end
    
    subgraph "Traitement & Analytics"
        C -->|Batch Read| D[Spark]
        D -->|Write| E[HDFS Parquet]
        D -->|Write| F[(PostgreSQL)]
    end
    
    subgraph "Viz & Orchestration"
        F --> G[Grafana]
        H[Airflow] -->|Trigger| D
    end

    style A fill:#f9f,stroke:#333
    style B fill:#ffa,stroke:#333
    style C fill:#aff,stroke:#333
    style D fill:#faa,stroke:#333
    style G fill:#9f9,stroke:#333
```

- **Langage** : Python 3.9
- **Message Broker** : Apache Kafka (Confluent Image)
- **Data Lake** : Hadoop HDFS (Namenode/Datanode)
- **Processing** : Apache Spark 3.5 (Master/Worker)
- **Orchestration** : Apache Airflow 2.6
- **Visualisation** : Grafana + PostgreSQL

---

## 3. Installation & D√©marrage (Quick Start)

### Pr√©requis
- Docker Desktop install√© et fonctionnel.
- Git.

### Lancement Rapide
1. **D√©marrer l'infrastructure**
   La commande suivante construit les images personnalis√©es et lance les 10+ conteneurs :
   ```bash
   docker-compose up -d --build
   ```
   > **Capture : Validation du D√©marrage**
   > ![Docker Compose Output](screenshots/1-docker_compose_up_output.png)

2. **V√©rifier l'√©tat des services**
   Tous les conteneurs doivent √™tre au statut "Running" (Vert).
   > **Capture : Vue Docker Desktop**
   > ![Docker Desktop](screenshots/2-docker-desktop_container_runing-view.png)

### Acc√®s aux Interfaces
| Service | URL | Identifiants |
|---------|-----|--------------|
| **Kafka** | `localhost:9092` | (TCP) |
| **HDFS WebUI** | [http://localhost:9870](http://localhost:9870) | - |
| **Spark Master** | [http://localhost:8080](http://localhost:8080) | - |
| **Airflow** | [http://localhost:8081](http://localhost:8081) | `admin` / `admin` |
| **Grafana** | [http://localhost:3000](http://localhost:3000) | `admin` / `admin` |

---

## 4. Rapport de Projet D√©taill√©

### √âtape 1 ‚Äî Collecte des Donn√©es (Data Collection)
Nous simulons un r√©seau de capteurs IoT urbains gr√¢ce au script `traffic_generator.py`.
- **M√©canisme** : Le script g√©n√®re des √©v√©nements al√©atoires r√©alistes bas√©s sur une liste de capteurs (lat/lon) d√©finie en dur.
- **Logique** : 
    - Simulation des heures de pointe (Vitesse r√©duite, densit√© √©lev√©e).
    - Format JSON standardis√©.
- **Exemple de Donn√©e JSON (Conforme au Sujet)** :
  ```json
  {
    "sensor_id": "S-001",
    "road_id": "R-001",
    "road_type": "autoroute",
    "zone": "Secteur-Nord",
    "vehicle_count": 85,
    "average_speed": 42.15,
    "occupancy_rate": 65.2,
    "event_time": "2024-01-04T10:00:00",
    "location": {"lat": 48.8566, "lon": 2.3522}
  }
  ```

### √âtape 2 ‚Äî Ingestion des donn√©es (Data Ingestion)
L'ingestion se fait en **Streaming** via Apache Kafka.
- **Topic** : `traffic-events`
- **Partitionnement** : Kafka g√®re la distribution. Dans notre cluster simple n≈ìud, nous utilisons 1 partition par d√©faut pour garantir l'ordre s√©quentiel.
- **Volume & Fr√©quence** : Le g√©n√©rateur envoie environ 1 √©v√©nement par seconde par capteur, ce qui permet de simuler un flux continu ("Unbounded Stream") sans saturer le disque local de d√©veloppement.
- **R√¥le de Kafka** : Agit comme un tampon (buffer) robuste. Si HDFS est lent ou indisponible, Kafka conserve les messages (Retention Policy).

### √âtape 3 ‚Äî Stockage des donn√©es brutes (Data Lake ‚Äì Raw Zone)
Les donn√©es sont d√©vers√©es dans **HDFS** dans la zone "Raw".
- **Composant** : Script `consumer_to_hdfs.py`.
- **Structure** : `/data/raw/traffic/date=YYYY-MM-DD/zone=NOM_ZONE/`
- **Justification** : Le double partitionnement (Date + Zone) est essentiel pour optimiser les requ√™tes analytiques de la municipalit√© qui travaille souvent par secteur g√©ographique.

> **Preuve de Stockage (WebHDFS)** :
> ![HDFS Web UI](screenshots/3-HDfs_web_ui_1.png)

### √âtape 4 ‚Äî Traitement des donn√©es (Data Processing)
Le traitement est un job Batch ex√©cut√© p√©riodiquement par **Apache Spark**.
- **Lancement** :
  ```powershell
  # IMPORTANT : Donner les droits d'√©criture √† Spark avant le premier lancement
  docker exec namenode hdfs dfs -chmod -R 777 /data

  # Execution du job
  docker exec spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --jars /opt/spark/jars/postgresql-42.2.18.jar /opt/spark-apps/traffic_processing.py
  ```
- **Op√©rations** :
  1. Lecture des fichiers JSON depuis HDFS Raw Zone.
  2. Typage correct des donn√©es (Timestamp, Double).
  3. **Calculs (Conformes au Sujet)** :
     - **Trafic moyen par zone** (Secteur-Nord, Centre, etc.)
     - **Vitesse moyenne par route** (`road_id`)
     - **Taux de congestion** bas√© sur l'occupation.
  4. Les donn√©es transform√©es sont archiv√©es et envoy√©es vers la Serving Layer.

> **Validation du Job Spark (Master UI)** :
> ![Spark Master UI](screenshots/4-Spark_masker-1.png)


### √âtape 5 ‚Äî Structuration analytique (Analytics Zone)
Les r√©sultats raffin√©s sont stock√©s dans deux destinations compl√©mentaires :

1. **HDFS (Format Parquet)** : `/data/analytics/traffic`
   - **Justification** : Le format Parquet est colonnaire et supporte la compression Snappy. Il est 10x plus rapide √† lire pour les requ√™tes analytiques que le JSON ou CSV.

   > **Preuve de Stockage (WebHDFS Analytics)** :
   > ![HDFS Analytics Zone](screenshots/5-HDfs_web_ui_2.png)

2. **PostgreSQL (Serving Layer)** : Tables `traffic_stats_zone` et `traffic_stats_road`
   - **Justification** : Permet une interrogation SQL rapide (latence < 100ms) pour servir les tableaux de bord en temps r√©el.
  ```powershell
  docker exec postgres psql -U airflow -d airflow -c "SELECT zone, window_start, avg_traffic, avg_speed, status FROM traffic_stats_zone ORDER BY window_start DESC LIMIT 10;"
  ```
  
   > **Donn√©es Agreg√©es dans Postgres** :
   > ![Postgres Data Evidence](screenshots/6-Postgres_data_1.png)

### √âtape 6 ‚Äî Exploitation et visualisation
**Grafana** (Port `3000`) est connect√© √† la base PostgreSQL pour afficher les KPIs de mobilit√©.

#### 1. Configuration de la Source de Donn√©es (PostgreSQL)
- **Host** : `postgres:5432`
- **Database** : `airflow`
- **User** : `airflow`
- **Password** : `airflow`
- **SSL Mode** : `disable`

> **Preuve de Configuration Source** :
> ![Grafana Config](screenshots/6-grafana-config.png)

#### 2. Cr√©ation du Dashboard (Panels)
Pour chaque graphique, cliquez sur **Add** -> **Visualization**, s√©lectionnez la source Postgres et entrez :

- **√âvolution du trafic par zone** (Type: *Time Series*) :
  ```sql
  SELECT window_start as time, avg_traffic as value, zone FROM traffic_stats_zone ORDER BY 1;
  ```
- **Vitesse moyenne par route** (Type: *Bar chart*) :
  ```sql
  SELECT road_id, road_avg_speed FROM traffic_stats_road;
  ```
- **Zones qui sont fluide ** (Type: *Table*) :
  ```sql
  SELECT zone, status FROM traffic_stats_zone WHERE status = 'FLUID';
  ```

> **Tableau de Bord Final (Vue d'ensemble)** :
> ![Grafana Dashboard Final](screenshots/6-grafana-dashboard.png)

### √âtape 7 ‚Äî Orchestration du pipeline
**Apache Airflow** (Port `8081`) automatise l'ensemble du flux de traitement. Sans orchestration, le pipeline n√©cessiterait une intervention humaine constante pour chaque nouveau fichier stock√©.

#### 1. Acc√®s et Configuration
- **Acc√®s UI** : `http://localhost:8081` (Admin / Admin)
- **Configuration de la Connexion Spark** (Crucial pour l'automatisation) :
  1. Allez dans **Admin** > **Connections**.
  2. Cliquez sur **+** (Add record).
  3. **Conn Id** : `spark_default`
  4. **Conn Type** : `Spark`
  5. **Host** : `spark://spark-master`
  6. **Port** : `7077`
  7. **Save**.

> **Preuve de Configuration de la Connexion Spark** :
> ![Airflow Spark Connection](screenshots/7-airflow-connection.png)

#### 2. Fonctionnement du DAG
- **DAG** : `traffic_pipeline_dag`
- **Fr√©quence** : `*/5 * * * *` (Toutes les 5 minutes).
- **Justification** : Airflow assure la **robustesse**. Si le job Spark √©choue (ex: probl√®me r√©seau), Airflow le d√©tecte, envoie une alerte et tente de le relancer automatiquement selon les `retries` configur√©s.

> **Supervision du Workflow (Airflow Grid)** :
> ![Airflow Web UI](screenshots/5-airflow-webui-1.png)

---

## 5. Conclusion & Enseignements

### Architecture Lambda Impl√©ment√©e
Ce projet d√©montre la mise en ≈ìuvre d'une **Architecture Lambda** simplifi√©e, permettant de concilier la pr√©cision du traitement batch avec la r√©activit√© n√©cessaire au monitoring urbain :
- **Batch Layer** : Stockage complet dans HDFS et traitement lourd via Spark pour garantir une "source de v√©rit√©" historique.
- **Speed Layer** : Ingestion streaming via Kafka permettant de capturer les √©v√©nements √† la milliseconde.
- **Serving Layer** : PostgreSQL agit comme zone de service ultra-rapide pour Grafana, √©vitant les latences de lecture directe sur le Data Lake.

### Enseignements Cl√©s
- **Ma√Ætrise de l'√©cosyst√®me Big Data** : Int√©gration de composants h√©t√©rog√®nes (Hadoop, Spark, Kafka, Airflow) via Docker.
- **Optimisation du Stockage** : Compr√©hension de l'importance du partitionnement par date et de l'efficacit√© du format colonnaire (**Parquet**) pour les analyses analytiques.
- **Orchestration et Supervision** : Mise en place d'Airflow pour automatiser un pipeline complexe, assurant la robustesse et la visibilit√© sur les √©checs √©ventuels.
- **Serving Layer** : Justification de l'utilisation d'une base relationnelle (**PostgreSQL**) en compl√©ment du Data Lake pour offrir une latence minimale aux outils de visualisation comme **Grafana**.
